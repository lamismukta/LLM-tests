# LLM Configuration
llm_providers:
  openai:
    default_model: "gpt-4o-mini"
    models:
      - "gpt-4o-mini"
      - "gpt-4o"
      - "gpt-4-turbo"
    temperature: 0.7
    max_tokens: 2000

# Pipeline Configuration
pipelines:
  one_shot:
    description: "Single prompt analysis"
    enabled: true
  
  chain_of_thought:
    description: "Step-by-step reasoning approach"
    enabled: true
  
  multi_layer:
    description: "Multi-stage analysis with refinement"
    enabled: true

# Analysis Configuration
analysis:
  output_format: "json"
  save_intermediate_results: true
  results_dir: "results"

